description: |-
  Version: 2020_07_13
  ⚠️配置前请核对版本号是否最新; 测试时注意核验字段值的数据准确性: 描述中有❤️标志的字段
proxy: default
normal_status_list:
  - '200'
login_id: ''
downloader: pycurl
min_data_length: 0
min_link_length: 1
timeout: 30
load_wait: 0
cache_ttl: 0
cache_after: 0
encoding: ''
headers: ''
download_script: ''
rows:
  - name: ❤️列表详情链接配置
    description: 注意保持字段与详情页对应
    locator: xpath
    expression: >-
      ./a[not(contains(translate(@href, "PDF", "pdf"), ".pdf") or
      contains(translate(@href, "XLS", "xls"), ".xls") or
      contains(translate(@href, "DOC", "doc"), ".doc") or
      contains(translate(@href, "ZIP", "zip"), ".zip") or
      contains(translate(@href, "RAR", "rar"), ".rar") or
      contains(translate(@href, "WPS", "wps"), ".wps")) and not(contains(@href,
      "file://"))]
    storage_id: -1
    if_match: ''
    if_url_match: ''
    fields:
      - name: web_site
        description: 来源网站
        data_type: varchar(1024)
        locator: hub
        expression: web_site
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_13
                  验证网站名是否复制带了空格
                  """
                  import re

                  if len(context) == len(context.strip()):
                      return True
                  return False
            fail_type: retry
        is_dedup_array: false
      - name: web_site_url
        description: 来源网站地址
        data_type: varchar(1024)
        locator: hub
        expression: web_site_url
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_13
                  验证链接是否合法
                  """
                  import re
                  from urllib.parse import urlparse

                  url = context.strip()
                  # 调度中可能复制带了空格
                  if len(url) != context:
                      return False

                  rules = [
                      r"(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]",
                      r"@^(https?|ftp)://[^\s/$.?#].[^\s]*$@iS",
                      r"#\b(([\w-]+://?|www[.])[^\s()<>]+(?:\([\w\d]+\)|([^[:punct:]\s]|/)))#iS",
                  ]
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(url)
                      if res:
                          return True
                  else:
                      try:
                          q = urlparse(url)
                      except:
                          return False
                      else:
                          if all([q.scheme, q.netloc, q.path]):
                              return True
                          else:
                              return False
            fail_type: retry
        is_dedup_array: false
      - name: refer
        description: "\U0001F494列表页链接上线需删除"
        data_type: varchar(128)
        locator: url_re
        expression: .*
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  验证链接是否合法
                  """
                  import re
                  from urllib.parse import urlparse

                  url = context.strip()
                  rules = [
                      r"(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]",
                      r"@^(https?|ftp)://[^\s/$.?#].[^\s]*$@iS",
                      r"#\b(([\w-]+://?|www[.])[^\s()<>]+(?:\([\w\d]+\)|([^[:punct:]\s]|/)))#iS",
                  ]
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(url)
                      if res:
                          return True
                  else:
                      try:
                          q = urlparse(url)
                      except:
                          return False
                      else:
                          if all([q.scheme, q.netloc, q.path]):
                              return True
                          else:
                              return False
            fail_type: retry
        is_dedup_array: false
      - name: title
        description: "\U0001F49A标题"
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  验证标题是否是省略过的
                  """
                  text = context.strip()
                  if text and text.endswith("..."):
                      return False
                  else:
                      return True
            fail_type: retry
        is_dedup_array: false
      - name: publish_time
        description: ❤️新闻发布时间
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors:
          - method: script
            params: |-
              def process(text):
                  """Version: 2020_07_13
                  时间提取脚本模版
                  """
                  import re
                  from datetime import datetime

                  data = text.strip()

                  rules = [
                      r"(\d{2}\d{2}([\.\-/|年月\s]{1,3}\d{1,2}){2}日?(\s?\d{2}:\d{2}(:\d{2})?)?)|(\d{1,2}\s?(分钟|小时|天)前)",  # 常见中文日期格式, 网上找的
                      # r"\d{10}",  # TODO: 处理时间戳, 遇到再加: 15开头的10或13位数字, 其实匹配前10个就够了
                      r"",  # 如有不是常见的日期时间格式，此处替换成案例
                  ]
                  # 无内容时间返回空
                  if not data:
                      return "error:空字符串"
                  # 预处理，替换掉会影响正则提取的固定字符串, 如点击量的数字
                  flags = [""]
                  for each in flags:
                      data = data.replace(each, "")
                  # 提取日期时间
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(data)
                      res = res and res[0] and res[0][0]
                      if res:
                           return res
                      else:
                          continue
                  else:
                      return f"error:{text}"
          - method: text2datetime
            params: ''
        validators:
          - method: script
            params: |
              def validate(context):
                  """Version: 2020_07_11
                  验证发布时间是否大于当前时间
                  """
                  from datetime import datetime

                  if context.startswith("error:"):
                      return False
                  else:
                      now = datetime.now()
                      pt = datetime.strptime(context, "%Y-%m-%d %H:%M:%S")
                      if pt > now:
                          return False
                      return True
            fail_type: retry
        is_dedup_array: false
      - name: tag
        description: ❤️标签
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: true
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors:
          - method: script
            params: |-
              def process(text):
                  """Version: 2020_07_12_19:10:00
                  标签提取脚本模版
                  returns:
                      []: 正则匹配失败
                  """
                  import re

                  tag = text.strip()

                  # 按需排序
                  rules = [
                      r"\b([\u4e00-\u9fa5]+)\b",  # 连续中文字符
                      # r"",  # 如有不是常见的作者格式，此处替换成案例
                  ]
                  # 无内容返回空列表
                  if not tag:
                      return []
                  # 预处理，替换掉会影响正则提取的固定字符串, 从验证器中更新
                  flags = ["标签", "关键字", "主题分类"]
                  for each in flags:
                      tag = tag.replace(each, "")
                  # 提取作者
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(tag)
                      if res:
                          return res
                      else:
                          continue
                  else:
                      return [f"error:{text}"]
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  TODO: 验证标签, 不同网站调整长度, 测试一段时间
                  """
                  import re

                  text = context.strip()
                  # 验证长度
                  if len(text) > 5 or len(text) < 2:
                      return False
                  # 验证是否是含有非中文字符
                  rules = [r"[\u4e00-\u9fa5]+"]
                  for each in rules:
                      p = re.compile(each)
                      res = p.match(text)
                      if res:
                          if res[0] == text:
                              return True
                          else:
                              return False
                  else:
                      return False
            fail_type: retry
        is_dedup_array: false
      - name: author
        description: ❤️作者
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: true
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors:
          - method: script
            params: |-
              def process(text):
                  """Version: 2020_07_12
                  作者提取脚本模版
                  returns:
                      []: 正则匹配失败
                  """
                  import re

                  # 按需排序
                  rules = [
                      r"\b([\u4e00-\u9fa5]\s?[\u4e00-\u9fa5]+)\b",  # 常见中文作者格式
                      r"([a-zA-Z0-9]+)",  # 作者为字母和数字组合
                      # r"",  # 如有不是常见的作者格式，此处替换成案例
                  ]
                  # 预处理，替换掉会影响正则提取的固定字符串, 从验证器中更新
                  flags = ["作者", "记者", "撰文", "通讯员", "责任编辑", "编辑", "通讯中"]
                  punctuations = ["【", "】", "（", "）"]
                  flags.extend(punctuations)
                  for each in flags:
                      text = text.replace(each, "")

                  # 判断是否是空作者
                  author = text.strip()
                  if len(author) < 2:
                      return []

                  # 提取作者
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(author)
                      if res:
                          return [i.replace(" ", "") for i in res]
                      else:
                          continue
                  else:
                      return [f"error:{text}"]
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  验证中文作者是否含有非法词
                  """
                  if context.startswith("error:"):
                      return False
                  elif len(context) > 4 or len(context) < 2:
                      return False
                  return True
            fail_type: retry
        is_dedup_array: false
      - name: author_info
        description: 作者简介
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators: []
        is_dedup_array: false
      - name: abstract
        description: 新闻摘要
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators: []
        is_dedup_array: false
    links:
      - page_rule_id: null
        new_task_name: ''
        description: 详情页链接
        locator: xpath
        expression: ./@href
        processors: []
        keep_other_site: false
  - name: "\U0001F4C1文件链接配置"
    description: 解析文件链接
    locator: xpath
    expression: >-
      ./a[(contains(translate(@href, "PDF", "pdf"), ".pdf") or
      contains(translate(@href, "XLS", "xls"), ".xls") or
      contains(translate(@href, "DOC", "doc"), ".doc") or
      contains(translate(@href, "ZIP", "zip"), ".zip") or
      contains(translate(@href, "RAR", "rar"), ".rar") or
      contains(translate(@href, "WPS", "wps"), ".wps")) and not(contains(@href,
      "file://"))]/href
    storage_id: 10270
    if_match: ''
    if_url_match: ''
    fields:
      - name: web_site
        description: 来源网站
        data_type: varchar(1024)
        locator: hub
        expression: web_site
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """2020_07_13
                  验证网站名是否复制带了空格
                  """
                  import re

                  if len(context) == len(context.strip()):
                      return True
                  return False
            fail_type: retry
        is_dedup_array: false
      - name: web_site_url
        description: 来源网站地址
        data_type: varchar(1024)
        locator: hub
        expression: web_site_url
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  验证链接是否合法
                  """
                  import re
                  from urllib.parse import urlparse

                  url = context.strip()
                  rules = [
                      r"(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]",
                      r"@^(https?|ftp)://[^\s/$.?#].[^\s]*$@iS",
                      r"#\b(([\w-]+://?|www[.])[^\s()<>]+(?:\([\w\d]+\)|([^[:punct:]\s]|/)))#iS",
                  ]
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(url)
                      if res:
                          return True
                  else:
                      try:
                          q = urlparse(url)
                      except:
                          return False
                      else:
                          if all([q.scheme, q.netloc, q.path]):
                              return True
                          else:
                              return False
            fail_type: retry
        is_dedup_array: false
      - name: content_md5
        description: 用来去重的md5
        data_type: varchar(1024)
        locator: hub
        expression: web_site
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: content_url
        description: 正文详情url
        data_type: varchar(1024)
        locator: xpath
        expression: ./@href
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: true
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  验证链接是否合法
                  """
                  import re
                  from urllib.parse import urlparse

                  url = context.strip()
                  rules = [
                      r"(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]",
                      r"@^(https?|ftp)://[^\s/$.?#].[^\s]*$@iS",
                      r"#\b(([\w-]+://?|www[.])[^\s()<>]+(?:\([\w\d]+\)|([^[:punct:]\s]|/)))#iS",
                  ]
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(url)
                      if res:
                          return True
                  else:
                      try:
                          q = urlparse(url)
                      except:
                          return False
                      else:
                          if all([q.scheme, q.netloc, q.path]):
                              return True
                          else:
                              return False
            fail_type: retry
        is_dedup_array: false
      - name: title
        description: 标题
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors:
          - method: script
            params: |-
              def process(text):
                  """
                  标题中的省略号在附件这块就不用title验证器了直接过, 链接更重要
                  """
                  return text
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: article_file_name
        description: 附件名称
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: true
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: true
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: article_file_url
        description: 附件地址
        data_type: varchar(1024)
        locator: xpath
        expression: ./@href
        is_dedup_key: false
        multi: true
        keep_html: false
        extend_url: true
        is_extra: false
        allow_invalid_expression: true
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  验证链接是否合法
                  """
                  import re
                  from urllib.parse import urlparse

                  url = context.strip()
                  rules = [
                      r"(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]",
                      r"@^(https?|ftp)://[^\s/$.?#].[^\s]*$@iS",
                      r"#\b(([\w-]+://?|www[.])[^\s()<>]+(?:\([\w\d]+\)|([^[:punct:]\s]|/)))#iS",
                  ]
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(url)
                      if res:
                          return True
                  else:
                      try:
                          q = urlparse(url)
                      except:
                          return False
                      else:
                          if all([q.scheme, q.netloc, q.path]):
                              return True
                          else:
                              return False
            fail_type: retry
        is_dedup_array: false
      - name: refer
        description: "\U0001F494列表页链接上线需删除"
        data_type: varchar(128)
        locator: url_re
        expression: .*
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  验证链接是否合法
                  """
                  import re
                  from urllib.parse import urlparse

                  url = context.strip()
                  rules = [
                      r"(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]",
                      r"@^(https?|ftp)://[^\s/$.?#].[^\s]*$@iS",
                      r"#\b(([\w-]+://?|www[.])[^\s()<>]+(?:\([\w\d]+\)|([^[:punct:]\s]|/)))#iS",
                  ]
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(url)
                      if res:
                          return True
                  else:
                      try:
                          q = urlparse(url)
                      except:
                          return False
                      else:
                          if all([q.scheme, q.netloc, q.path]):
                              return True
                          else:
                              return False
            fail_type: retry
        is_dedup_array: false
      - name: news_type
        description: "\U0001F49A新闻类型"
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors:
          - method: script
            params: |-
              def process(text):
                  """
                  处理文本, 返回处理后的文本
                  """
                  return text
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  验证新闻类型
                  """
                  import re

                  text = context.strip()
                  # 验证长度
                  if len(text) > 5 or len(text) < 2:
                      return False

                  rules = [r"[\u4e00-\u9fa5]+"]
                  for each in rules:
                      p = re.compile(each)
                      res = p.match(text)
                      if res:
                          if res[0] == text:
                              return True
                          else:
                              return False
                  else:
                      return False
            fail_type: retry
        is_dedup_array: false
      - name: publish_time
        description: ❤️新闻发布时间
        data_type: datetime
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors:
          - method: script
            params: |-
              def process(text):
                  """Version: 2020_07_13
                  时间提取脚本模版
                  """
                  import re
                  from datetime import datetime

                  data = text.strip()

                  rules = [
                      r"(\d{2}\d{2}([\.\-/|年月\s]{1,3}\d{1,2}){2}日?(\s?\d{2}:\d{2}(:\d{2})?)?)|(\d{1,2}\s?(分钟|小时|天)前)",  # 常见中文日期格式, 网上找的
                      # r"\d{10}",  # TODO: 处理时间戳, 遇到再加: 15开头的10或13位数字, 其实匹配前10个就够了
                      r"",  # 如有不是常见的日期时间格式，此处替换成案例
                  ]
                  # 无内容时间返回空
                  if not data:
                      return "error:空字符串"
                  # 预处理，替换掉会影响正则提取的固定字符串, 如点击量的数字
                  flags = [""]
                  for each in flags:
                      data = data.replace(each, "")
                  # 提取日期时间
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(data)
                      res = res and res[0] and res[0][0]
                      if res:
                           return res
                      else:
                          continue
                  else:
                      return f"error:{text}"
          - method: text2datetime
            params: ''
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  验证发布时间是否大于当前时间
                  """
                  from datetime import datetime

                  if context.startswith("error:"):
                      return False
                  else:
                      now = datetime.now()
                      pt = datetime.strptime(context, "%Y-%m-%d %H:%M:%S")
                      if pt > now:
                          return False
                      return True
            fail_type: retry
        is_dedup_array: false
      - name: publish_org
        description: ❤️新闻发布来源
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors:
          - method: script
            params: |-
              def process(text):
                  """Version: 2020_07_13_23:53:00
                  来源提取
                  """
                  import re

                  # 预处理，替换掉会影响正则提取的固定字符串, 从验证器中更新
                  flags = ["文章来源", "信息来源", "来源", "转自", "发文机关"]
                  for each in flags:
                      text = text.replace(each, "")

                  # 来源为空
                  if len(text.strip()) < 2:
                      return ""

                  # 按需排序
                  rules = [
                      r"([\u4e00-\u9fa5]+)",  # 默认提取中文, 其它格式卡住后处理
                      # r"", # 自定义
                      # r"([^\s/$.?].[^\s]*)", # www.railwaygazette.com
                  ]

                  # 提取来源
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(text)
                      if res:
                          return res[0]
                      else:
                          continue
                  else:
                      return f"error:[{text}]"
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  验证来源中是否有error: 
                      卡住未经正确处理的请求
                  """
                  if context.startswith("error:"):
                      return False
                  return True
            fail_type: retry
        is_dedup_array: false
      - name: author
        description: ❤️作者
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: true
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors:
          - method: script
            params: |-
              def process(text):
                  """Version: 2020_07_12
                  作者提取脚本模版
                  returns:
                      []: 正则匹配失败
                  """
                  import re

                  # 按需排序
                  rules = [
                      r"\b([\u4e00-\u9fa5]\s?[\u4e00-\u9fa5]+)\b",  # 常见中文作者格式
                      r"([a-zA-Z0-9]+)",  # 作者为字母和数字组合
                      # r"",  # 如有不是常见的作者格式，此处替换成案例
                  ]
                  # 预处理，替换掉会影响正则提取的固定字符串, 从验证器中更新
                  flags = ["作者", "记者", "撰文", "通讯员", "责任编辑", "编辑", "通讯中"]
                  punctuations = ["【", "】", "（", "）"]
                  flags.extend(punctuations)
                  for each in flags:
                      text = text.replace(each, "")

                  # 判断是否是空作者
                  author = text.strip()
                  if len(author) < 2:
                      return []

                  # 提取作者
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(author)
                      if res:
                          return [i.replace(" ", "") for i in res]
                      else:
                          continue
                  else:
                      return [f"error:{text}"]
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  验证中文作者是否含有非法词
                  """
                  if context.startswith("error:"):
                      return False
                  elif len(context) > 4 or len(context) < 2:
                      return False
                  return True
            fail_type: retry
        is_dedup_array: false
      - name: tag
        description: ❤️标签
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: true
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors:
          - method: script
            params: |-
              def process(text):
                  """Version: 2020_07_12_19:10:00
                  标签提取脚本模版
                  returns:
                      []: 正则匹配失败
                  """
                  import re

                  tag = text.strip()

                  # 按需排序
                  rules = [
                      r"\b([\u4e00-\u9fa5]+)\b",  # 连续中文字符
                      # r"",  # 如有不是常见的作者格式，此处替换成案例
                  ]
                  # 无内容返回空列表
                  if not tag:
                      return []
                  # 预处理，替换掉会影响正则提取的固定字符串, 从验证器中更新
                  flags = ["标签", "关键字", "主题分类"]
                  for each in flags:
                      tag = tag.replace(each, "")
                  # 提取作者
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(tag)
                      if res:
                          return res
                      else:
                          continue
                  else:
                      return [f"error:{text}"]
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  TODO: 验证标签, 不同网站调整长度, 测试一段时间
                  """
                  import re

                  text = context.strip()
                  # 验证长度
                  if len(text) > 5 or len(text) < 2:
                      return False
                  # 验证是否是含有非中文字符
                  rules = [r"[\u4e00-\u9fa5]+"]
                  for each in rules:
                      p = re.compile(each)
                      res = p.match(text)
                      if res:
                          if res[0] == text:
                              return True
                          else:
                              return False
                  else:
                      return False
            fail_type: retry
        is_dedup_array: false
      - name: abstract
        description: 新闻摘要
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
    processors:
      - method: script
        params: |-
          # 计算content_md5, 数组字段去重, 不处理附件
          import hashlib


          def md5(text):
              return hashlib.md5(str(text).encode()).hexdigest()


          def process(data):
              """Version: 2020_07_12
              计算 网站名 发布时间 标题 内容详情 的MD5
              """
              keys = ["web_site", "publish_time", "title", "content"]
              values = [data.get(k) or "" for k in keys]

              data["content_md5"] = md5("".join(values))
              # 数组字段去重, 去空
              if data.get("tag"):
                  data["tag"] = list(set([tag for tag in data["tag"] if tag]))
              if data.get("author"):
                  data["author"] = list(set([tag for tag in data["author"] if tag]))

              return data
    links: []
preprocessors: []
prevalidators:
  - method: regex
    params: '^(?!.*[200|code]).*$'
    fail_type: success
    succ_type: ignore
    target: status_code
examples:
  - method: GET
    url: ''
    data: ''
    description: "\U0001F44D配置测试保存做对比"
    should_save_sync: false
  - method: GET
    url: ''
    data: ''
    description: 异常测试说明：
    should_save_sync: false
  - method: GET
    url: ''
    data: ''
    description: 异常测试说明：
    should_save_sync: false
size: small
global_dedup: false
captcha: {}
use_bot: false
