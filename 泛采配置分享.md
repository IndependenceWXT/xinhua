

# 分享说明

个人配置经验和一些其他爬虫经验分享 也不太算培训 不作硬性要求

分享的目的 减少配置时间上可能的浪费 让大家少走弯路 加快进度

需要说的比较杂乱，只是简单的归类

1.  一些介绍
2.  配置一般流程
3.  配置演示
4.  排错演示

我负责大家配置的进度和质量

# 项目相关介绍

负责人：刘博

其他配置人员：唐浩 王小婷 房天生

项目：智库简介 新华财经即本项目需求个人分析

基本思路：多线程编程的思路  不影响进度 处理影响了进度的一切问题

思路用于项目也用于配置工作

网站多的项目很难遇到的 xpath 推测泛采如何实现的 经验积累

总量2000 配置人员3+5

`4*8*60=1920` s型成长曲线

一两周的缓冲达到每人每天4个配置

10月份需要配置完， 最终交付是12月

# 前提

1.  了解基本简单的爬虫爬取流程: 有列表页详情页概念即可, 会python尤佳.

2.  xpath会用基本的即可: 配置中常用的

    ```python
    #例如
    # .//*[@class="xxx"]  ./@href text()
    # 函数
    # contains, not, 
    # 其他
    # ancestor, preceding, following
    ```
    
3. 反面教材配置示范智库212

4. 默认大家对爬虫是有了解有兴趣的, 写过爬虫采集过网站, 会xpath, 能写简单的python处理字符串

# 泛采系统定位

没有编程和爬虫经验的人也能短时间内快速配置采集大量的简单网站

# 个人泛采配置常用工具

chrome插件: [xpath helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl), [Proxy SwitchyOmega](https://chrome.google.com/webstore/detail/proxy-switchyomega/padekgcemlokbadohgkifijomclgjgif), [开发工具箱](https://chrome.google.com/webstore/detail/mflanociobpenleccopmoanpdbcjcanm), [stylus](https://chrome.google.com/webstore/detail/stylus/clngdbkpkpeebahjckkjfobafhncgmne), [EditThisCookies](https://chrome.google.com/webstore/detail/edit-this-cookie/fngmhnnpilhplaeedifhccceomclgfbg), [JsonView](https://chrome.google.com/webstore/detail/chklaanhfefbnpoihckbnefhakgolnmc)

软件: vscode(编写脚本), EditPlus(处理链接), Sublime Text(查看保存配置模版yaml文件)

自己建一个文件夹 如 xinhua 里面放模版文件 脚本处理器python文件 核验文档模版 问题记录

# 泛采配置一般流程

1.  需求分析: 新项目开始的时候了解需要采集的字段
2.  网站调研: 新配置组开始的时候, 分析了解需要采集的部分的网站列表页详情页结构，包括列表页的分页，详情页链接的大概或者准确数量，为数据量核验做准备. 
3.  开始配置: 根据调研结果配置调度，列表页和详情页配置
4.  启动调度: 在调度运行出错的时候排除错误, 保存的配置会很快生效. 该过程可进行数据准确性校验. 如发布时间是否超出当前的时间, 作者, 来源, 标签中是否正确采集.
5.  数据校验, 数据量核验: 根据需求文档的栏目核验数据量, 排错并适当重新运行调度采集缺失的部分数据. 常见错误: 调度页码配置少了 少了首页或者末页导致数据少, 列表页xpath定位不准确导致数据少或多
6.  步骤4的过程中开始下一个网站的配置.



# 常用泛采功能

### 1.  调度计划
调度配置
    定时方式 历史调度和更新调度的区别 不同的部分 更新调度保证当天能跑完就行
    抓取速率 一般默认 1 2 5 10
    种子链接类型 -> 测试种子
    页面配置: 默认的链接返回的响应对应的页面解析规则配置
数据展示区
    手动触发
    启用计划
    日志
        日志流
        日志搜索
            时间 当天 双击确定 搜索 等一会儿 翻到最后一页
            搜索结果
                时间｜系统方法｜调度ID｜**规则ID**｜脚本｜行号 调用方法 **信息**
                task_index
                行名称加个颜色的emoji方便快速定位
    刷新按钮
批次信息
    批次id run_id
    批次日期 一个批次日期内的链接任务会去重复, 重跑调度的时候需要废弃之前的批次并删除
    批次状态
    完成数总数
    操作
        缓存队列
            task_id task_index 状态
            操作
                extra 请求对象的文档
                    referer
                编辑 不能用
                删除 1451 [发改委](https://www.ndrc.gov.cn/yjzx/yjzx_add_fgs.jsp?SiteId=318)
        废弃批次
        删除批次



**以下页面规则中传递处理的都是字符串**

### 2.  列表页页面规则

页面下载配置
页面去重配置
页面抽取配置
抽取结果检验
测试用例: *一定要保存一个正常的通过测试的链接, 有异常页面出现的时候用来对比, 异常链接处理后用正常的测试再泡跑一下, 避免改错了*
    方法
    是否存储
    url

### 3.  详情页页面规则
页面抽取配置
    抽取行
    导入行
        名称
        描述
        内容匹配正则
        URL匹配正则
        存储配置  详情页使用配置 列表页不使用存储
        定位方式
        表达式
        操作
            编辑
            删除
            导出
测试结果展示区
    日志
        pipline
        dedup
        load_rule
        global dedup
        crawl_doc.proxy
        download
        validate stage 正常状态码 页面校验器
        decode 页面编码
        extract
            preprocess
                fn: remove_xml remove_control_character
                页面预处理器
            row extract 遍历行配置 根据定位规则提取全部字段 再运行每个字段的处理器
        normalize 处理字段值
        file process 下载文件
        new_link
        validate_result stage 抽取结果校验
        save 是否存储
    结果列表
    请求信息
    页面内容
        页面
        源码 下载方式的不同会导致响应结果不同跟chrome里不一样, 配置会失效

# 字段配置
导入字段
字段
    字段名称 一定要与数据库表字段一致
    描述
    定位方式
    表达式
    操作
       编辑
           消重
           数组
           保存HTML
           拓展url
           附加字段
           允许无效表达式 出发之后不会执行处理器和验证
           下载资源
       移动排序
       删除
       导出
   展开
      处理器
      验证

# 定位方式
当前页面 行 全文html 字段 行xpath定位提取的html字符串
填充默认值 固定值建议从调度中用hub_fields传值
URL正则 从url中提取数据 时间
内容正则 从html源码中正则方式提取字段值
抽取json字段 jsonpath 行 定位到列表result.data 字段 node.title
xpath 非必要都用这个
脚本 行 字段 略有不同
列表页值 hub_fields字典中的key
区域内图片抽取 现在项目用不到 类似取img里的@src后urljoin当前链接

# 字段处理器方法
正则(表达式, 分组)  "(.*?)" 1
替换(源字符串, 目标字符串)  "来源" ""
去除前后空格(无参数)  同python里的str.strip
格式化(目标字符串)  开始{}
抽取json字段
取MD5
脚本
时间格式化  将常见时间(2018年2月3日)转为 1990-01-01 00:00:00格式
去掉多余节点  drop_tree

# 验证器

长度 1 +inf retry

# 脚本处理器

经常使用的库 datatime re

# 其他

## 系统无法处理的问题

链接任务无法完成的 比如需要翻墙的请求 无法得到响应的
有无法处理的反爬的

## 隐藏的可能问题

chrome里的页面和系统里的会不一样



## 配置思路

调度生成请求->列表页->详情页

每个网站的样式大致一样



## 网站调研

分析需要采集的部分的列表页和详情页, 分析确定那些列表页, 那些是详情页

大部分都是包含需要采集的具体字段信息的页面为详情页

所有包含详情页链接的都可视为列表页

调研阶段根据页数, 每页详情链接数, 最后一页的详情链接数确定记录数据总量

一般看一下最新的几个链接, 中间页的几个链接, 和最后一页的链接, 三种页面的样式有无巨大的不同可确定详情页的种类

可根据内容部分的标签的class, 按内容正则分行配置规则, 也可后续出错阶段配置, 任务不会丢失



## 配置模板解释

### 1. 调度模板

默认脚本方式, 清晰明确方便维护和配置

### 2. 列表页模板

不保存, 详情页没有的字段在这里设置, 列表页包含文件的另建新行

列表页含有文件链接的需要配置后重跑调度

### 3. 详情页模板

默认只处理200响应, 特殊状态码出错后再处理

固定值字段从调度中传值, 减少出错概率

行的名称加个emoji方便查看日志时快速定位

验证器可以丰富一点



# 总结自己的模版

7天之前自己总结一下之后统一

配置完成到未通过最终审核可以上线的 控制在10个以内



# website

先采集近3年全部历史

xpath中使用正则



fx168
中国新闻网
中国气象局
中国社会科学院
中国科学院
中国经济网
中国经营网
中国能源网
中国证券监督管理委员会
中国金融四十人论坛
交通运输部
人力资源和社会保障部
人民网
人民银行
住房和城乡建设部
农业农村部
华夏时报
司法部
商务部
国务院侨务办公室
国务院发展研究中心
国务院国有资产监督管理委员会
国务院新闻办公室
国家体育总局
国家医疗保障局
国家原子能机构
国家发展和改革委员会
国家国防科技工业局
国家宗教事务局
国家广播电视总局
国家文物局
国家新闻出版署（国家版权局）
国家机关事务管理局
国家林业和草原局
国家标准化管理委员会
国家民族事务委员会
国家知识产权局
国家税务总局
国家统计局
国家航天局
国家行政学院
国家语言文字工作委员会
国家邮政局
外交部
央广网
审计署
教育部
新京报网
新华网
每日经济新闻
民政部
海关总署
澎湃新闻
科学技术部
第一财经
腾讯科技
腾讯网
自然资源部
证券日报
证券时报快讯
证券时报网
财政部





1869  加reffer重跑

1564 js跑不动

http://jndpc.jinan.gov.cn/art/2020/7/8/art_2202_4520112.html?xxgkhide=1