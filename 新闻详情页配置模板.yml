description: >-
  Version: 2020_07_11_13_20_00

  âš ï¸é…ç½®å‰è¯·æ ¸å¯¹ç‰ˆæœ¬å·æ˜¯å¦æœ€æ–°; å¸¦æœ‰âœ”ï¸å­—æ®µå¼€å§‹å¾€ä¸‹çš„å­—æ®µéœ€è¦é…ç½®, é¡µé¢ä¸­æ²¡æœ‰çš„å­—æ®µåˆ é™¤å³å¯; æµ‹è¯•æ—¶æ³¨æ„æ ¸éªŒå­—æ®µå€¼çš„æ•°æ®å‡†ç¡®æ€§:
  æè¿°ä¸­æœ‰â¤ï¸æ ‡å¿—çš„å­—æ®µ
proxy: default
normal_status_list:
  - '200'
  - '404'
login_id: ''
downloader: pycurl
min_data_length: 1
min_link_length: 0
timeout: 30
load_wait: 0
cache_ttl: 0
cache_after: 0
encoding: ''
headers: ''
download_script: ''
rows:
  - name: â¤ï¸è¯¦æƒ…é…ç½®
    description: æ³¨æ„ä¿æŒå­—æ®µä¸åˆ—è¡¨é¡µå¯¹åº”
    locator: self
    expression: ''
    storage_id: 10270
    if_match: ''
    if_url_match: ''
    fields:
      - name: content_md5
        description: ç”¨äºå»é‡çš„md5
        data_type: varchar(1024)
        locator: hub
        expression: web_site
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: web_site
        description: æ¥æºç½‘ç«™
        data_type: varchar(1024)
        locator: hub
        expression: web_site
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: web_site_url
        description: æ¥æºç½‘ç«™åœ°å€
        data_type: varchar(1024)
        locator: hub
        expression: web_site_url
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  éªŒè¯é“¾æ¥æ˜¯å¦åˆæ³•
                  """
                  import re
                  from urllib.parse import urlparse

                  url = context.strip()
                  rules = [
                      r"(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]",
                      r"@^(https?|ftp)://[^\s/$.?#].[^\s]*$@iS",
                      r"#\b(([\w-]+://?|www[.])[^\s()<>]+(?:\([\w\d]+\)|([^[:punct:]\s]|/)))#iS",
                  ]
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(url)
                      if res:
                          return True
                  else:
                      try:
                          q = urlparse(url)
                      except:
                          return False
                      else:
                          if all([q.scheme, q.netloc, q.path]):
                              return True
                          else:
                              return False
            fail_type: retry
        is_dedup_array: false
      - name: refer
        description: åˆ—è¡¨é¡µé“¾æ¥
        data_type: varchar(128)
        locator: hub
        expression: refer
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: true
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  éªŒè¯é“¾æ¥æ˜¯å¦åˆæ³•
                  """
                  import re
                  from urllib.parse import urlparse

                  url = context.strip()
                  rules = [
                      r"(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]",
                      r"@^(https?|ftp)://[^\s/$.?#].[^\s]*$@iS",
                      r"#\b(([\w-]+://?|www[.])[^\s()<>]+(?:\([\w\d]+\)|([^[:punct:]\s]|/)))#iS",
                  ]
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(url)
                      if res:
                          return True
                  else:
                      try:
                          q = urlparse(url)
                      except:
                          return False
                      else:
                          if all([q.scheme, q.netloc, q.path]):
                              return True
                          else:
                              return False
            fail_type: retry
        is_dedup_array: false
      - name: content_url
        description: æ­£æ–‡è¯¦æƒ…url
        data_type: varchar(1024)
        locator: url_re
        expression: .*
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  éªŒè¯é“¾æ¥æ˜¯å¦åˆæ³•
                  """
                  import re
                  from urllib.parse import urlparse

                  url = context.strip()
                  rules = [
                      r"(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]",
                      r"@^(https?|ftp)://[^\s/$.?#].[^\s]*$@iS",
                      r"#\b(([\w-]+://?|www[.])[^\s()<>]+(?:\([\w\d]+\)|([^[:punct:]\s]|/)))#iS",
                  ]
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(url)
                      if res:
                          return True
                  else:
                      try:
                          q = urlparse(url)
                      except:
                          return False
                      else:
                          if all([q.scheme, q.netloc, q.path]):
                              return True
                          else:
                              return False
            fail_type: retry
        is_dedup_array: false
      - name: title
        description: æ ‡é¢˜
        data_type: varchar(1024)
        locator: hub
        expression: title
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  éªŒè¯æ ‡é¢˜æ˜¯å¦æ˜¯çœç•¥è¿‡çš„
                  """
                  text = context.strip()
                  if text and text.endswith("..."):
                      return False
                  else:
                      return True
            fail_type: retry
        is_dedup_array: false
      - name: publish_time
        description: æ–°é—»å‘å¸ƒæ—¶é—´
        data_type: datetime
        locator: hub
        expression: publish_time
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors:
          - method: script
            params: |-
              def process(text):
                  """Version: 2020_07_11
                  æ—¶é—´æå–è„šæœ¬æ¨¡ç‰ˆ
                  """
                  import re
                  from datetime import datetime

                  data = text.strip()

                  rules = [
                      r"\d{4}[-å¹´/]\d{1,2}[-æœˆ/]\d{1,2}[-æ—¥/]?[\s\d{2}:\d{2}[:\d{2}]?]?",  # å¸¸è§ä¸­æ–‡æ—¥æœŸæ ¼å¼
                      # r"\d{10}",  # TODO: å¤„ç†æ—¶é—´æˆ³, é‡åˆ°å†åŠ : 15å¼€å¤´çš„10æˆ–13ä½æ•°å­—, å…¶å®åŒ¹é…å‰10ä¸ªå°±å¤Ÿäº†
                      r"",  # å¦‚æœ‰ä¸æ˜¯å¸¸è§çš„æ—¥æœŸæ—¶é—´æ ¼å¼ï¼Œæ­¤å¤„æ›¿æ¢æˆæ¡ˆä¾‹
                  ]
                  # æ— å†…å®¹æ—¶é—´è¿”å›ç©º
                  if not data:
                      return "error:ç©ºå­—ç¬¦ä¸²"
                  # é¢„å¤„ç†ï¼Œæ›¿æ¢æ‰ä¼šå½±å“æ­£åˆ™æå–çš„å›ºå®šå­—ç¬¦ä¸², å¦‚ç‚¹å‡»é‡çš„æ•°å­—
                  flags = [""]
                  for each in flags:
                      data = data.replace(each, "")
                  # æå–æ—¥æœŸæ—¶é—´
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(data)
                      if res:
                          return res[0]
                      else:
                          continue
                  else:
                      return f"error:{text}"
          - method: text2datetime
            params: ''
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  éªŒè¯å‘å¸ƒæ—¶é—´æ˜¯å¦å¤§äºå½“å‰æ—¶é—´
                  """
                  from datetime import datetime

                  if context.startswith("error:"):
                      return False
                  else:
                      now = datetime.now()
                      pt = datetime.strptime(context, "%Y-%m-%d %H:%M:%S")
                      if pt > now:
                          return False
                      return True
            fail_type: retry
        is_dedup_array: false
      - name: news_type
        description: "\U0001F49Aæ–°é—»ç±»å‹ï¼ˆèµ„è®¯ï¼Œå…¬å‘Šï¼Œçº°æ¼ï¼‰"
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: publish_org
        description: â¤ï¸æ–°é—»å‘å¸ƒæ¥æº
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors:
          - method: script
            params: |-
              def process(text):
                  """Version: 2020_07_11
                  æ¥æºæå–
                  """
                  import re

                  # æŒ‰éœ€æ’åº
                  rules = [
                      r"([\u4e00-\u9fa5]+)",  # é»˜è®¤æå–ä¸­æ–‡, å…¶å®ƒæ ¼å¼å¡ä½åå¤„ç†
                      # r"", # è‡ªå®šä¹‰
                      # r"([^\s/$.?].[^\s]*)", # www.railwaygazette.com
                  ]
                  # æ— å†…å®¹ä½œè€…è¿”å›ç©ºåˆ—è¡¨
                  if not text.strip():
                      return []
                  # é¢„å¤„ç†ï¼Œæ›¿æ¢æ‰ä¼šå½±å“æ­£åˆ™æå–çš„å›ºå®šå­—ç¬¦ä¸², ä»éªŒè¯å™¨ä¸­æ›´æ–°
                  flags = ["æ¥æº", "è½¬è‡ª"]
                  for each in flags:
                      text = text.replace(each, "")
                  # æå–æ¥æº
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(text)
                      if res:
                          return res[0]
                      else:
                          continue
                  else:
                      return f"error:[{text}]"
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  éªŒè¯æ¥æºä¸­æ˜¯å¦æœ‰error: 
                      å¡ä½æœªç»æ­£ç¡®å¤„ç†çš„è¯·æ±‚
                  """
                  if context.startswith("error:"):
                      return False
                  return True
            fail_type: retry
        is_dedup_array: false
      - name: tag
        description: â¤ï¸æ ‡ç­¾
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: true
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  TODO: éªŒè¯æ ‡ç­¾, ä¸åŒç½‘ç«™è°ƒæ•´é•¿åº¦, æµ‹è¯•ä¸€æ®µæ—¶é—´
                  """
                  import re

                  text = context.strip()
                  # éªŒè¯é•¿åº¦
                  if len(text) > 5 or len(text) < 2:
                      return False
                  # éªŒè¯æ˜¯å¦æ˜¯å«æœ‰éä¸­æ–‡å­—ç¬¦
                  rules = [r"[\u4e00-\u9fa5]+"]
                  for each in rules:
                      p = re.compile(each)
                      res = p.match(text)
                      if res:
                          if res[0] == text:
                              return True
                          else:
                              return False
                  else:
                      return False
            fail_type: retry
        is_dedup_array: false
      - name: author
        description: â¤ï¸ä½œè€…
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: true
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors:
          - method: script
            params: |-
              def process(text):
                  """Version: 2020_07_11
                  ä½œè€…æå–è„šæœ¬æ¨¡ç‰ˆ
                  returns:
                      []: æ­£åˆ™åŒ¹é…å¤±è´¥
                  """
                  import re

                  author = text.strip()

                  # æŒ‰éœ€æ’åº
                  rules = [
                      r"\b([\u4e00-\u9fa5]\s?[\u4e00-\u9fa5]+)\b",  # å¸¸è§ä¸­æ–‡ä½œè€…æ ¼å¼
                      r"([a-zA-Z0-9]+)",  # ä½œè€…ä¸ºå­—æ¯å’Œæ•°å­—ç»„åˆ
                      # r"",  # å¦‚æœ‰ä¸æ˜¯å¸¸è§çš„ä½œè€…æ ¼å¼ï¼Œæ­¤å¤„æ›¿æ¢æˆæ¡ˆä¾‹
                  ]
                  # æ— å†…å®¹ä½œè€…è¿”å›ç©ºåˆ—è¡¨
                  if not author:
                      return []
                  # é¢„å¤„ç†ï¼Œæ›¿æ¢æ‰ä¼šå½±å“æ­£åˆ™æå–çš„å›ºå®šå­—ç¬¦ä¸², ä»éªŒè¯å™¨ä¸­æ›´æ–°
                  flags = ["è®°è€…", "æ’°æ–‡", "é€šè®¯å‘˜", "è´£ä»»ç¼–è¾‘", "ç¼–è¾‘", "é€šè®¯ä¸­"]
                  for each in flags:
                      author = author.replace(each, "")
                  # æå–ä½œè€…
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(author)
                      if res:
                          return [i.replace(" ", "") for i in res]
                      else:
                          continue
                  else:
                      return [f"error:{text}"]
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  éªŒè¯ä¸­æ–‡ä½œè€…æ˜¯å¦å«æœ‰éæ³•è¯
                  """
                  if context.startswith("error:"):
                      return False
                  elif len(context) > 4 or len(context) < 2:
                      return False
                  return True
            fail_type: retry
        is_dedup_array: false
      - name: author_info
        description: ä½œè€…ç®€ä»‹
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: abstract
        description: æ–°é—»æ‘˜è¦
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: content
        description: æ–°é—»è¯¦æƒ…
        data_type: text
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors:
          - method: script
            params: |-
              def process(text):
                  """Version: 2020_07_11_release
                  å¤„ç†çŸ­æ­£æ–‡: ç”¨äº†æŒºä¹…äº†, åº”è¯¥ç¨³å®šå¯ç”¨äº†
                  """
                  import re

                  if len(re.sub(r"\s+", "", text)) < 80:
                      return ""
                  return text
        validators: []
        is_dedup_array: false
      - name: content_html
        description: æ–°é—»è¯¦æƒ…å¸¦htmlæ ‡ç­¾
        data_type: text
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: true
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: article_file_name
        description: é™„ä»¶åç§°
        data_type: varchar(1024)
        locator: xpath
        expression: >-
          .//a[(contains(translate(@href, "PDF", "pdf"), ".pdf") or
          contains(translate(@href, "XLS", "xls"), ".xls") or
          contains(translate(@href, "DOC", "doc"), ".doc")) and
          not(contains(@href, "file://"))]
        is_dedup_key: false
        multi: true
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: true
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: article_file_url
        description: é™„ä»¶åœ°å€
        data_type: varchar(1024)
        locator: xpath
        expression: >-
          .//a[(contains(translate(@href, "PDF", "pdf"), ".pdf") or
          contains(translate(@href, "XLS", "xls"), ".xls") or
          contains(translate(@href, "DOC", "doc"), ".doc")) and
          not(contains(@href, "file://"))]/@href
        is_dedup_key: false
        multi: true
        keep_html: false
        extend_url: true
        is_extra: false
        allow_invalid_expression: true
        download_policy: not_download
        processors: []
        validators:
          - method: script
            params: |-
              def validate(context):
                  """Version: 2020_07_11
                  éªŒè¯é“¾æ¥æ˜¯å¦åˆæ³•
                  """
                  import re
                  from urllib.parse import urlparse

                  url = context.strip()
                  rules = [
                      r"(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]",
                      r"@^(https?|ftp)://[^\s/$.?#].[^\s]*$@iS",
                      r"#\b(([\w-]+://?|www[.])[^\s()<>]+(?:\([\w\d]+\)|([^[:punct:]\s]|/)))#iS",
                  ]
                  for each in rules:
                      p = re.compile(each)
                      res = p.findall(url)
                      if res:
                          return True
                  else:
                      try:
                          q = urlparse(url)
                      except:
                          return False
                      else:
                          if all([q.scheme, q.netloc, q.path]):
                              return True
                          else:
                              return False
            fail_type: retry
        is_dedup_array: false
      - name: likes
        description: ç‚¹èµæ•°
        data_type: bigint
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: hits
        description: ç‚¹å‡»é‡è®¿é—®é‡
        data_type: bigint
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: replies
        description: è¯„è®ºæ•°
        data_type: bigint
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: office_account
        description: å…¬ä¼—å·
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
      - name: office_account_info
        description: å…¬ä¼—å·ç®€ä»‹
        data_type: varchar(1024)
        locator: xpath
        expression: .
        is_dedup_key: false
        multi: false
        keep_html: false
        extend_url: false
        is_extra: false
        allow_invalid_expression: false
        download_policy: not_download
        processors: []
        validators:
          - method: length
            params: 1 +inf
            fail_type: retry
        is_dedup_array: false
    processors:
      - method: script
        params: |-
          import hashlib


          def md5(text):
              return hashlib.md5(str(text).encode()).hexdigest()


          def process(data):
              """2020_07_11
              è®¡ç®—ç½‘ç«™åå‘å¸ƒæ—¶é—´æ ‡é¢˜å†…å®¹è¯¦æƒ…çš„MD5
              """
              data["content_md5"] = md5(
                  data["web_site"]
                  + data["publish_time"]
                  + data["title"]
                  + data.get("content", "")  # ç©ºå­—ç¬¦ä¸²åˆ°è¿™é‡Œä¼šå˜æˆNone
              )
              return data
    links: []
preprocessors: []
prevalidators:
  - method: regex
    params: '^(?!.*[200|200]).*$'
    fail_type: success
    succ_type: ignore
    target: status_code
examples:
  - method: GET
    url: ''
    data: ''
    description: ğŸ‘é…ç½®æµ‹è¯•ä¿ç•™æµ‹è¯•é“¾æ¥å‹¿åˆ 
    should_save_sync: false
  - method: GET
    url: ''
    data: ''
    description: 'å¼‚å¸¸æµ‹è¯•è¯´æ˜:'
    should_save_sync: false
  - method: GET
    url: ''
    data: ''
    description: 'å¼‚å¸¸æµ‹è¯•è¯´æ˜:'
    should_save_sync: false
size: small
global_dedup: false
captcha: {}
use_bot: false
